# =============================================================================
# Fedora User Scripts + CHIMERA AI Stack — Portainer Stack
# =============================================================================
# Deploy via Portainer: Stacks → Add Stack → Upload/Paste this file
#
# This stack includes:
#   1. Fedora User Scripts   — Web UI for managing/scheduling bash scripts
#   2. Ollama (IPEX-LLM)     — LLM inference on Intel Arc 770 via SYCL
#   3. Open WebUI            — Chat interface for local AI
#
# Hardware target: Intel Arc 770 (16GB) · AMD 7700 · 32GB RAM · Fedora 43
#
# For scripts that need HOST access (Shadow Ops, Dev Toolkit, etc.),
# switch to Mode 2 below by commenting/uncommenting.
# =============================================================================

services:
  # =========================================================================
  # FEDORA USER SCRIPTS — Script Manager Web UI
  # =========================================================================
  # MODE 1: Standard (default) — scripts run INSIDE the container
  fedora-user-scripts:
    build:
      context: .
      dockerfile: Dockerfile
    image: chimera/fedora-user-scripts:latest
    container_name: fedora-user-scripts
    restart: unless-stopped
    ports:
      - "${FUS_PORT:-9855}:9855"
    volumes:
      - fus-scripts:/data/scripts
    environment:
      - FUS_PORT=9855
      - FUS_SCRIPTS_DIR=/data/scripts
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:9855/"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    labels:
      - "com.chimera.service=fedora-user-scripts"
      - "com.chimera.role=script-manager"

  # MODE 2: Host Access — scripts can modify the HOST system
  # To use: comment out the service above and uncomment this one.
  # fedora-user-scripts:
  #   build:
  #     context: .
  #     dockerfile: Dockerfile
  #   image: chimera/fedora-user-scripts:latest
  #   container_name: fedora-user-scripts
  #   restart: unless-stopped
  #   privileged: true
  #   network_mode: host
  #   pid: host
  #   volumes:
  #     - fus-scripts:/data/scripts
  #     - /:/host:rw
  #     - /var/run/dbus:/var/run/dbus
  #     - /run/systemd:/run/systemd
  #   environment:
  #     - FUS_PORT=9855
  #     - FUS_SCRIPTS_DIR=/data/scripts
  #     - FUS_HOST_ROOT=/host
  #   healthcheck:
  #     test: ["CMD", "curl", "-sf", "http://localhost:9855/"]
  #     interval: 30s
  #     timeout: 5s
  #     retries: 3
  #     start_period: 10s
  #   labels:
  #     - "com.chimera.service=fedora-user-scripts"
  #     - "com.chimera.role=script-manager"
  #     - "com.chimera.mode=host-access"

  # =========================================================================
  # OLLAMA — LLM Inference via IPEX-LLM (Intel Arc SYCL acceleration)
  # =========================================================================
  # Uses Intel's official IPEX-LLM image for optimal performance on Arc GPUs.
  # The SYCL backend is ~2x faster than Vulkan on Intel Arc.
  # =========================================================================
  ollama:
    image: intelanalytics/ipex-llm-inference-cpp-xpu:latest
    container_name: ollama-ipex
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    devices:
      - /dev/dri:/dev/dri
    environment:
      # Force all model layers onto Intel Arc GPU
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_GPU=999
      # Keep model loaded in VRAM permanently (no 5-min timeout)
      - OLLAMA_KEEP_ALIVE=-1
      # One model at a time (16GB VRAM budget)
      - OLLAMA_MAX_LOADED_MODELS=1
      # Allow 4 parallel requests per model
      - OLLAMA_NUM_PARALLEL=4
      # Enable Flash Attention for faster inference
      - OLLAMA_FLASH_ATTENTION=1
      # Intel GPU / SYCL / Level Zero settings
      - ZES_ENABLE_SYSMAN=1
      - SYCL_CACHE_PERSISTENT=1
      - SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1
      - no_proxy=localhost,127.0.0.1
    # 16GB shared memory for model loading
    shm_size: "16g"
    command: >
      bash -c "cd /llm/ollama &&
               export OLLAMA_NUM_GPU=999 &&
               ./ollama serve"
    healthcheck:
      test: ["CMD", "bash", "-c", "curl -sf http://localhost:11434/api/tags || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    labels:
      - "com.chimera.service=ollama"
      - "com.chimera.role=llm-inference"
      - "com.chimera.gpu=intel-arc-770"

  # =========================================================================
  # OPEN WEBUI — Chat Interface for Local AI
  # =========================================================================
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    ports:
      - "3000:8080"
    volumes:
      - open-webui-data:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      # No auth for local use (single-user desktop)
      - WEBUI_AUTH=false
      - ENABLE_SIGNUP=false
      - WEBUI_NAME=CHIMERA AI
      - DEFAULT_MODELS=hf.co/bartowski/JOSIEFIED-Qwen3-8B-GGUF:Q8_0
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8080/"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s
    labels:
      - "com.chimera.service=open-webui"
      - "com.chimera.role=chat-interface"

  # =========================================================================
  # MODEL SEEDER — Pulls AI models on first deploy (runs once, then exits)
  # =========================================================================
  model-seeder:
    image: curlimages/curl:latest
    container_name: model-seeder
    depends_on:
      ollama:
        condition: service_healthy
    restart: "no"
    entrypoint: /bin/sh
    command:
      - -c
      - |
        echo "=== CHIMERA Model Seeder ==="
        echo ""
        echo "[1/3] Pulling JOSIEFIED-Qwen3-8B (abliterated + fine-tuned, primary)..."
        curl -sf http://ollama:11434/api/pull -d '{"name":"hf.co/bartowski/JOSIEFIED-Qwen3-8B-GGUF:Q8_0"}' --max-time 3600 | head -c 4096
        echo ""
        echo "[2/3] Pulling NeuralDaredevil-8B (abliterated, secondary)..."
        curl -sf http://ollama:11434/api/pull -d '{"name":"closex/neuraldaredevil-8b-abliterated"}' --max-time 3600 | head -c 4096
        echo ""
        echo "[3/3] Pulling Qwen2.5-Coder:7B (code assistant)..."
        curl -sf http://ollama:11434/api/pull -d '{"name":"qwen2.5-coder:7b"}' --max-time 3600 | head -c 4096
        echo ""
        echo "=== All models pulled. CHIMERA AI is ready. ==="
    labels:
      - "com.chimera.service=model-seeder"
      - "com.chimera.role=init"

volumes:
  fus-scripts:
    driver: local
  ollama-data:
    driver: local
  open-webui-data:
    driver: local
