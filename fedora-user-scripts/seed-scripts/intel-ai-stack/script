#!/bin/bash
# ==============================================================================
# PROJECT CHIMERA | INTEL ARC 770 AI STACK
# ==============================================================================
# TARGET:    Fedora 43 Desktop
# HARDWARE:  Intel Arc 770 16GB · AMD 7700 (8C/16T) · 32GB RAM
# ROLE:      Full local AI — Ollama (IPEX-LLM/SYCL) + Open WebUI
# MODEL:     JOSIEFIED-Qwen3:8B (abliterated + fine-tuned, best uncensored)
# ==============================================================================
#
# Architecture:
#   ┌──────────────────────────────────────────────────────┐
#   │  Open WebUI (:3000)  ←── Browser Chat Interface      │
#   │       ↕  Ollama API                                   │
#   │  Ollama/IPEX-LLM (:11434)  ←── SYCL → Intel Arc 770 │
#   │       ↕  /dev/dri                                     │
#   │  Intel GPU Compute Runtime  ←── Level Zero + oneAPI   │
#   └──────────────────────────────────────────────────────┘
#
# ==============================================================================

set -e

REAL_USER=${SUDO_USER:-$USER}
USER_HOME=$(getent passwd "$REAL_USER" | cut -d: -f6)

RED='\033[0;31m'
GREEN='\033[0;32m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
YELLOW='\033[1;33m'
PURPLE='\033[0;35m'
NC='\033[0m'

if [[ $EUID -ne 0 ]]; then
    echo -e "${RED}[!] Run as root: sudo bash <script>${NC}"
    exit 1
fi

echo -e "${PURPLE}"
cat << "EOF"
  ╔══════════════════════════════════════════════════╗
  ║   CHIMERA AI STACK · Intel Arc 770 Edition       ║
  ║   Ollama + IPEX-LLM + Open WebUI                ║
  ║   JOSIEFIED-Qwen3:8B (Abliterated)              ║
  ╚══════════════════════════════════════════════════╝
EOF
echo -e "${NC}"

# ==============================================================================
# PRE-FLIGHT CHECKS
# ==============================================================================
echo -e "${BLUE}[PRE-FLIGHT]${NC} Verifying hardware..."

# Check for Intel Arc GPU
if lspci | grep -qi "VGA.*Intel.*Arc\|Display.*Intel.*Arc"; then
    echo -e "  ${GREEN}✓${NC} Intel Arc GPU detected"
    lspci | grep -i "VGA\|Display" | grep -i Intel | head -1
else
    echo -e "  ${YELLOW}[!]${NC} No Intel Arc GPU detected in lspci"
    echo -e "  ${YELLOW}    Continuing anyway — the GPU may still work with the drivers${NC}"
fi

# Check /dev/dri exists
if [ -d /dev/dri ]; then
    echo -e "  ${GREEN}✓${NC} /dev/dri exists"
    ls -la /dev/dri/
else
    echo -e "  ${RED}[!]${NC} /dev/dri not found — GPU may not be initialized"
fi

# Check RAM
TOTAL_RAM_GB=$(free -g | awk '/Mem:/ {print $2}')
echo -e "  ${GREEN}✓${NC} RAM: ${TOTAL_RAM_GB}GB detected"

# Check BIOS setting reminder
echo -e "  ${YELLOW}[!]${NC} IMPORTANT: Ensure ReBAR / 'Above 4G Decoding' is ENABLED in BIOS"
echo -e "      (Required for Intel Arc GPU compute)"
echo ""

# ==============================================================================
# PHASE 1: INTEL GPU COMPUTE DRIVERS & RUNTIME
# ==============================================================================
echo -e "${BLUE}[1/6]${NC} Installing Intel GPU compute runtime..."

# Install Intel GPU packages from Fedora repos
dnf install -y \
    intel-gpu-tools \
    intel-compute-runtime \
    level-zero \
    level-zero-devel \
    intel-opencl \
    clinfo \
    intel-media-driver \
    mesa-dri-drivers \
    mesa-vulkan-drivers \
    vulkan-tools \
    2>/dev/null || true

# Install Intel oneAPI components needed for SYCL
echo -e "  Installing Intel oneAPI runtime components..."

# Add Intel oneAPI repository
if [ ! -f /etc/yum.repos.d/oneAPI.repo ]; then
    cat > /tmp/intel-oneapi.repo << 'INTELREPO'
[oneAPI]
name=Intel oneAPI repository
baseurl=https://yum.repos.intel.com/oneapi
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://yum.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB
INTELREPO
    mv /tmp/intel-oneapi.repo /etc/yum.repos.d/oneAPI.repo
fi

# Install only the runtime components needed (not the full toolkit)
dnf install -y \
    intel-oneapi-runtime-dpcpp-cpp \
    intel-oneapi-runtime-mkl \
    2>/dev/null || true

# Add user to render and video groups for GPU access
usermod -aG render "$REAL_USER" 2>/dev/null || true
usermod -aG video "$REAL_USER" 2>/dev/null || true

echo -e "  ${GREEN}✓${NC} Intel GPU compute runtime installed"

# Verify GPU is accessible
echo -e "  Verifying GPU compute..."
if command -v clinfo &>/dev/null; then
    DEVICES=$(clinfo --list 2>/dev/null | grep -i "intel\|arc" | head -3)
    if [ -n "$DEVICES" ]; then
        echo -e "  ${GREEN}✓${NC} GPU compute devices found:"
        echo "$DEVICES" | while read -r line; do echo "      $line"; done
    else
        echo -e "  ${YELLOW}[!]${NC} No Intel compute devices found via clinfo (may need reboot)"
    fi
fi

# ==============================================================================
# PHASE 2: DOCKER/PODMAN SETUP
# ==============================================================================
echo -e "\n${BLUE}[2/6]${NC} Ensuring container runtime is ready..."

# Prefer Docker if available, fall back to Podman
CONTAINER_RT="docker"
if ! command -v docker &>/dev/null; then
    if command -v podman &>/dev/null; then
        CONTAINER_RT="podman"
        echo -e "  Using Podman as container runtime"
    else
        echo -e "  Installing Docker..."
        dnf config-manager addrepo --from-repofile=https://download.docker.com/linux/fedora/docker-ce.repo 2>/dev/null || true
        dnf install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin 2>/dev/null || true
        systemctl enable --now docker
        usermod -aG docker "$REAL_USER"
    fi
else
    echo -e "  Using Docker as container runtime"
fi

# Ensure Docker is running
if [ "$CONTAINER_RT" = "docker" ]; then
    systemctl start docker 2>/dev/null || true
fi
echo -e "  ${GREEN}✓${NC} Container runtime ($CONTAINER_RT) ready"

# ==============================================================================
# PHASE 3: DEPLOY OLLAMA VIA IPEX-LLM (SYCL-ACCELERATED)
# ==============================================================================
echo -e "\n${BLUE}[3/6]${NC} Deploying Ollama with IPEX-LLM (Intel Arc SYCL acceleration)..."

# Create persistent data directory
OLLAMA_DATA="$USER_HOME/.ollama"
mkdir -p "$OLLAMA_DATA"
chown -R "$REAL_USER:$REAL_USER" "$OLLAMA_DATA"

# Stop existing Ollama containers
$CONTAINER_RT stop ollama-ipex 2>/dev/null || true
$CONTAINER_RT rm ollama-ipex 2>/dev/null || true

# Pull the IPEX-LLM Ollama image (Intel's official optimized build)
echo -e "  Pulling IPEX-LLM Ollama image (this may take a while)..."
$CONTAINER_RT pull intelanalytics/ipex-llm-inference-cpp-xpu:latest

# Launch Ollama with Intel Arc GPU
echo -e "  Starting Ollama container with Intel Arc GPU..."
$CONTAINER_RT run -d \
    --name ollama-ipex \
    --restart unless-stopped \
    --device /dev/dri \
    --device /dev/dri/renderD128 \
    -v "$OLLAMA_DATA:/root/.ollama" \
    -p 11434:11434 \
    -e OLLAMA_HOST=0.0.0.0 \
    -e OLLAMA_NUM_GPU=999 \
    -e OLLAMA_KEEP_ALIVE=-1 \
    -e OLLAMA_MAX_LOADED_MODELS=1 \
    -e OLLAMA_NUM_PARALLEL=4 \
    -e OLLAMA_FLASH_ATTENTION=1 \
    -e ZES_ENABLE_SYSMAN=1 \
    -e SYCL_CACHE_PERSISTENT=1 \
    -e SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1 \
    -e no_proxy=localhost,127.0.0.1 \
    --shm-size=16g \
    intelanalytics/ipex-llm-inference-cpp-xpu:latest \
    bash -c "cd /llm/ollama && export OLLAMA_NUM_GPU=999 && ./ollama serve"

# Wait for Ollama to be ready
echo -e "  Waiting for Ollama to start..."
for i in $(seq 1 30); do
    if curl -sf http://localhost:11434/api/tags &>/dev/null; then
        echo -e "  ${GREEN}✓${NC} Ollama is running on port 11434"
        break
    fi
    sleep 2
done

# Verify GPU is being used
echo -e "  Checking GPU status in container..."
$CONTAINER_RT exec ollama-ipex bash -c 'test -d /dev/dri && echo "GPU device accessible"' 2>/dev/null || true

echo -e "  ${GREEN}✓${NC} Ollama (IPEX-LLM) deployed with SYCL acceleration"

# ==============================================================================
# PHASE 4: PULL ABLITERATED AI MODELS
# ==============================================================================
echo -e "\n${BLUE}[4/6]${NC} Pulling AI models (optimized for 16GB VRAM)..."

# Function to pull a model via the Ollama API
pull_model() {
    local model=$1
    local desc=$2
    echo -e "  ${CYAN}Pulling:${NC} $model — $desc"
    curl -sf http://localhost:11434/api/pull \
        -d "{\"name\": \"$model\"}" \
        --max-time 1800 \
        | while IFS= read -r line; do
            status=$(echo "$line" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('status',''))" 2>/dev/null)
            if [ -n "$status" ]; then
                printf "\r  %-70s" "$status"
            fi
        done
    echo ""
}

# PRIMARY: JOSIEFIED-Qwen3:8B — Top abliterated model (abliteration + DPO fine-tune)
# - Perfect 10/10 adherence score on UGI Leaderboard
# - Maintains coherence in extended conversations (unlike pure abliteration models)
# - Fits comfortably in 16GB VRAM at Q4_K_M (~5GB) or Q8_0 (~8.5GB)
pull_model "hf.co/bartowski/JOSIEFIED-Qwen3-8B-GGUF:Q8_0" \
    "Abliterated + fine-tuned Qwen3 8B (Q8_0, ~8.5GB, best quality)"

# SECONDARY: NeuralDaredevil-8B — Best uncensored 8B on Open LLM Leaderboard
pull_model "closex/neuraldaredevil-8b-abliterated" \
    "DPO-tuned abliterated Daredevil 8B (best MMLU score)"

# CODING MODEL: Qwen2.5-Coder — Excellent for dev assistance
pull_model "qwen2.5-coder:7b" \
    "Qwen 2.5 Coder 7B (code generation, debugging, explanation)"

echo -e "  ${GREEN}✓${NC} Models pulled and ready"

# ==============================================================================
# PHASE 5: DEPLOY OPEN WEBUI
# ==============================================================================
echo -e "\n${BLUE}[5/6]${NC} Deploying Open WebUI (chat interface)..."

$CONTAINER_RT stop open-webui 2>/dev/null || true
$CONTAINER_RT rm open-webui 2>/dev/null || true

WEBUI_DATA="$USER_HOME/.open-webui"
mkdir -p "$WEBUI_DATA"
chown -R "$REAL_USER:$REAL_USER" "$WEBUI_DATA"

$CONTAINER_RT run -d \
    --name open-webui \
    --restart unless-stopped \
    -p 3000:8080 \
    -v "$WEBUI_DATA:/app/backend/data" \
    -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
    -e WEBUI_AUTH=false \
    -e ENABLE_SIGNUP=false \
    -e DEFAULT_MODELS="hf.co/bartowski/JOSIEFIED-Qwen3-8B-GGUF:Q8_0" \
    -e WEBUI_NAME="CHIMERA AI" \
    --add-host=host.docker.internal:host-gateway \
    ghcr.io/open-webui/open-webui:main

echo -e "  Waiting for Open WebUI..."
for i in $(seq 1 30); do
    if curl -sf http://localhost:3000 &>/dev/null; then
        echo -e "  ${GREEN}✓${NC} Open WebUI running on port 3000"
        break
    fi
    sleep 2
done

echo -e "  ${GREEN}✓${NC} Open WebUI deployed — http://localhost:3000"

# ==============================================================================
# PHASE 6: SYSTEM INTEGRATION & SHORTCUTS
# ==============================================================================
echo -e "\n${BLUE}[6/6]${NC} Integrating AI into Fedora desktop..."

# Create desktop shortcut for Open WebUI
DESKTOP_DIR="$USER_HOME/Desktop"
mkdir -p "$DESKTOP_DIR"

cat > "$DESKTOP_DIR/chimera-ai.desktop" << 'DESKTOP'
[Desktop Entry]
Name=CHIMERA AI
Comment=Local AI Chat (JOSIEFIED-Qwen3 on Intel Arc 770)
Exec=xdg-open http://localhost:3000
Icon=applications-science
Terminal=false
Type=Application
Categories=Science;ArtificialIntelligence;
StartupNotify=true
DESKTOP
chmod +x "$DESKTOP_DIR/chimera-ai.desktop"
chown "$REAL_USER:$REAL_USER" "$DESKTOP_DIR/chimera-ai.desktop"

# Also add to applications menu
mkdir -p /usr/share/applications
cp "$DESKTOP_DIR/chimera-ai.desktop" /usr/share/applications/chimera-ai.desktop

# Create CLI convenience aliases
SHELL_RC="$USER_HOME/.bashrc"
# Also add to .zshrc if it exists
for rc in "$USER_HOME/.bashrc" "$USER_HOME/.zshrc"; do
    if [ -f "$rc" ] && ! grep -q "# CHIMERA AI ALIASES" "$rc"; then
        cat >> "$rc" << 'AIALIASES'

# CHIMERA AI ALIASES
alias ai='xdg-open http://localhost:3000'
alias ai-chat='curl -sN http://localhost:11434/api/chat -d "{\"model\":\"hf.co/bartowski/JOSIEFIED-Qwen3-8B-GGUF:Q8_0\",\"messages\":[{\"role\":\"user\",\"content\":\"$*\"}]}"'
alias ai-models='curl -s http://localhost:11434/api/tags | python3 -m json.tool'
alias ai-status='echo "Ollama:"; curl -sf http://localhost:11434/ && echo " OK" || echo " DOWN"; echo "WebUI:"; curl -sf http://localhost:3000 > /dev/null && echo " OK" || echo " DOWN"'
alias ai-pull='curl -s http://localhost:11434/api/pull -d "{\"name\":\"$1\"}"'
alias ollama-logs='docker logs -f ollama-ipex 2>/dev/null || podman logs -f ollama-ipex'
alias webui-logs='docker logs -f open-webui 2>/dev/null || podman logs -f open-webui'
AIALIASES
        chown "$REAL_USER:$REAL_USER" "$rc"
    fi
done

# Create systemd user service to ensure containers start on login
mkdir -p "$USER_HOME/.config/systemd/user"
cat > "$USER_HOME/.config/systemd/user/chimera-ai.service" << AISERVICE
[Unit]
Description=CHIMERA AI Stack (Ollama + Open WebUI)
After=docker.service

[Service]
Type=oneshot
RemainAfterExit=yes
ExecStart=/usr/bin/$CONTAINER_RT start ollama-ipex open-webui
ExecStop=/usr/bin/$CONTAINER_RT stop open-webui ollama-ipex

[Install]
WantedBy=default.target
AISERVICE
chown -R "$REAL_USER:$REAL_USER" "$USER_HOME/.config/systemd/user"
sudo -u "$REAL_USER" systemctl --user daemon-reload 2>/dev/null || true
sudo -u "$REAL_USER" systemctl --user enable chimera-ai.service 2>/dev/null || true

echo -e "  ${GREEN}✓${NC} Desktop shortcut, CLI aliases, and autostart configured"

# ==============================================================================
# COMPLETION
# ==============================================================================
echo ""
echo -e "${GREEN}╔══════════════════════════════════════════════════════════════════╗${NC}"
echo -e "${GREEN}║           CHIMERA AI STACK — DEPLOYMENT COMPLETE                 ║${NC}"
echo -e "${GREEN}╚══════════════════════════════════════════════════════════════════╝${NC}"
echo ""
echo -e "${PURPLE}Hardware:${NC}"
echo -e "  GPU:  Intel Arc 770 (16GB VRAM, SYCL/Level Zero)"
echo -e "  CPU:  AMD 7700 (8C/16T)"
echo -e "  RAM:  ${TOTAL_RAM_GB}GB"
echo ""
echo -e "${CYAN}Services:${NC}"
echo -e "  Ollama (IPEX-LLM):  http://localhost:11434  (SYCL-accelerated)"
echo -e "  Open WebUI:          http://localhost:3000   (Chat interface)"
echo ""
echo -e "${CYAN}Models Installed:${NC}"
echo -e "  ★ JOSIEFIED-Qwen3-8B (Q8_0)     — Primary, abliterated + fine-tuned"
echo -e "    NeuralDaredevil-8B-Abliterated  — Secondary, best MMLU uncensored"
echo -e "    Qwen2.5-Coder:7B               — Code assistant"
echo ""
echo -e "${CYAN}Quick Commands:${NC}"
echo -e "  ai           — Open chat in browser"
echo -e "  ai-status    — Check if services are running"
echo -e "  ai-models    — List installed models"
echo -e "  ollama-logs  — Stream Ollama logs"
echo ""
echo -e "${YELLOW}Performance Tips:${NC}"
echo -e "  • Models load on first use — expect 10-20s initial load"
echo -e "  • OLLAMA_KEEP_ALIVE=-1 keeps model in VRAM (no reloading)"
echo -e "  • 8B Q8_0 uses ~8.5GB of 16GB VRAM — room for larger context"
echo -e "  • For longer contexts, try Q4_K_M quantization (~5GB VRAM)"
echo ""
echo -e "${PURPLE}CHIMERA AI STATUS: ${GREEN}OPERATIONAL${NC}"
echo ""
